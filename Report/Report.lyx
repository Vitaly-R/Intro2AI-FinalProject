#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Introduction to Artificial Intelligence 
\begin_inset Newline newline
\end_inset

Final Project - Durak with Reinforcement Learning
\end_layout

\begin_layout Author
Vitaly Rajev, Ziv Mahluf, Idan Yamin, Eyal Diskin
\end_layout

\begin_layout Date
Hebrew University of Jerusalem
\begin_inset Newline newline
\end_inset

August 2020
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Table of contents
\end_layout

\begin_layout Paragraph*
1.
 Introduction
\end_layout

\begin_layout Paragraph*
2.
 Rules of the Game
\end_layout

\begin_layout Paragraph*
3.
 Approach and Methods
\end_layout

\begin_layout Paragraph*
4.
 Training and Results
\end_layout

\begin_layout Paragraph*
5.
 Conclusions
\end_layout

\begin_layout Paragraph*
6.
 How to Run the Code
\end_layout

\begin_layout Paragraph*
7.
 References
\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

Introduction
\end_layout

\begin_layout Standard
Durak (Russian: дурак, meaning: 'fool') is a popular russian card game.
 The objective of the game is to get rid of all one's cards when the deck
 has no cards remaining in it.
 The last player which has cards in their hand loses and is the 'durak'
 (fool).
\begin_inset Newline newline
\end_inset

Durak is a flexible game with regards to the rules with which it can be
 played, and there are many variations of the game as a result, including
 ones which allow cheating (and it's the players' resposibility to notice
 in time), attacks of multiple cards, addition of cards after the round
 is over (in case of failure to defend), and more.
\begin_inset Newline newline
\end_inset

In this project, we've implemented AI agents for a simple variation of the
 game using reinforcement learning approaches.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Rules of the Game
\end_layout

\begin_layout Standard
Durak is traditionally played by two to six players with a deck of 36 cards,
 which results from a standard 52-card deck from which all cards with values
 2 to 5 were removed.
\begin_inset Newline newline
\end_inset

Before the cards are dealt, the deck is shuffled, and a random card is revealed
 from the deck, the suit of the card (hearts, diamonds, spades, clubs) determine
s the 'trump suit' for the game, and the revealed card is then plaed at
 the bottom of the deck (usually face-up).
\begin_inset Newline newline
\end_inset

After shuffling the deck, each player is dealt 6 cards, and if all starting
 hands are legal, then the game begins, otherwise, the deck is reshuffled
 and the cards are redealt as needed.
 A starting hand is considered legal if there are no 5 cards or more with
 the same suit in it.
\begin_inset Newline newline
\end_inset

After dealing the cards, the player with the lowest-valued trump card is
 attacking first.
\begin_inset Newline newline
\end_inset

Each game of durak consists of rounds, each consisting of attacks and defences.
 A round starts with the attacking player placing an attacking card.
 The defending player is then required to place a defending card, which
 is a card with the same suit and a higher value, or, if the attacking card
 is not a trump card, any trump card.
 If the defending player is unable or unwilling to defend, they take all
 cards currently on the table.
 After the first attack, attacking players can only place cards with values
 that appear on the table for an attack.
 After the defender plays a defending card, each player, strating from the
 attacker, and going clockwise, except the defender, has a chance to play
 an attacking card.
 If the player sdoes not attack, the next player has a chance, and if the
 player attacks with a card, the defender has a chance to defend or take
 the cards.
 If not player attacked, or the number of cards the defender defended from
 is 6 (the original hand size), or the defender has no cards remaining in
 their hand, the cards on the table are discarded and the players draw cards
 from the deck until they have 6 cards, starting from the first attacker,
 by order of attack chance, and ending with the defender.
 After drawing cards, the defender becomes the first attacker, and a new
 round begins.
 Any player with an empty hand at the end of the round is considered a winner
 and is removed from the game, and the attacker is chosen to be the next
 player clockwise (if needed).
 
\begin_inset Newline newline
\end_inset

The game ends when the deck is empty and there is at most one player with
 cards in their hand.
 If there is only one player, he is considered the loser (the 'durak'),
 and if there are no players with cards in their hands (for example, if
 the attacker and the defender had one card each and were the last players,
 and the defender defended from the attacking card) the game is considered
 to have no loser.
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Part*
Approach and Methods
\end_layout

\begin_layout Standard
For this game, we've decided to use reinforcement learning techniques to
 train the AI agents with two different algorithms.
 NFSP learns using the Q value, while PPO learns using the Advantage.
 The use of a neural network to approximate the Q-values of a state different
 actions was chosen since the number of possible states in Durak is too
 large to store in memory, even without taking into account memory regarding
 the cards in other players' hands, the cards in the current player's hands,
 and discarded cards.
\end_layout

\begin_layout Paragraph*
DQN Agent:
\end_layout

\begin_layout Standard
This agent is trained using the Q-Learning algorithm on a neural network.
 The neural network takes a representation of a state as an input, and outputs
 a vector of the size of the action space, in which each value corresponds
 to the expected reward for doing the action corresponding to the position
 of the value.
 The weights and biases of the network are then updated by applying the
 update formula - 
\begin_inset Formula $Q(s_{t},a_{t})\leftarrow Q(s_{t},a_{t})+\alpha\cdot(r_{t}+\gamma\cdot max_{a}Q(s_{t+1},a)-Q(s_{t},a_{t}))$
\end_inset

 (where 
\begin_inset Formula $\alpha$
\end_inset

 is the learning rate, 
\begin_inset Formula $\gamma$
\end_inset

 is the discount factor, and 
\begin_inset Formula $max_{a}Q(s_{t+1},a)$
\end_inset

 is the maximal possible expected reward from the next state) - to the output
 of the network, and using gradient descent to update the weights and biases
 themselves.
 
\end_layout

\begin_layout Paragraph*
PPO Agent:
\end_layout

\begin_layout Standard
The agent is trained using the Advantage value of the neural network, which
 is calculated after the game is finished.
 The calculated loss of the network is a linear combination of 3 different
 losses - the entropy loss, the value function loss, and the 
\begin_inset Quotes eld
\end_inset

conservative policy iteration
\begin_inset Quotes erd
\end_inset

 loss.
 Further information about the loss evaluation can be found at (Henry Charleswor
th, 2018).
 The reason we use these types of losses is to prevent changes that are
 too big in the gradients, while encouraging exploration.
 
\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

Training and Results
\end_layout

\begin_layout Standard
————————————————
\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

Conclusions
\end_layout

\begin_layout Standard
———————————————–
\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

How to Run the Code
\end_layout

\begin_layout Standard
———————————————
\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

References 
\end_layout

\begin_layout Standard
———————————————–
\end_layout

\begin_layout Part*
\begin_inset Newpage pagebreak
\end_inset

BONUS PART - DANK MEMEZ
\end_layout

\begin_layout Standard
————————————————–
\end_layout

\end_body
\end_document
